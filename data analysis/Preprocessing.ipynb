{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0169014-cb7d-457a-8c1d-58e4f0e750d3",
   "metadata": {},
   "source": [
    "# 5 Essential Machine Learning Techniques to Master Your Data Preprocessing\n",
    "\n",
    "https://medium.com/p/e888f6d220e1\n",
    "\n",
    "This blog explores five crucial preprocessing techniques that every data scientist must master: handling missing data, scaling and normalization, encoding categorical data, feature engineering, and dealing with imbalanced data. \n",
    "\n",
    "## 1. Handling Missing Data\n",
    "\n",
    "If improperly handled, missing data can lead to biased model predictions, misleading insights, or even training failures. \n",
    "\n",
    "Types of Missing Data\n",
    "\n",
    "    1. Missing Completely at Random (MCAR):\n",
    "        The probability of a missing data point is unrelated to any other observed or unobserved sample. In this case, removing the data may not introduce bias, as it’s random.\n",
    "    2. Missing at Random (MAR):\n",
    "        The missingness of a data point depends on other observed variables but not on the missing value itself. This is common in surveys or demographic datasets, where missing income data might be related to education level.\n",
    "    3. Missing Not at Random (MNAR):\n",
    "        The missingness is related to the unobserved data itself. For example, high-income people might be less likely to disclose their earnings, which can bias the dataset if not handled carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd907052-cb60-42d1-a11f-22965f7fda9a",
   "metadata": {},
   "source": [
    "**Strategy 1:** Listwise Deletion (Removing Missing Data)\n",
    "\n",
    "The simplest way to handle missing data is to remove rows containing missing values. While this works for small datasets with few missing entries, it is far less practical for large datasets where missing data is frequent, as it will yield loss of valuable information. (Básicamente usar el .dropna()\n",
    "\n",
    "Best Practice. Use listwise deletion cautiously. It’s only suitable when missing data is MCAR or removing rows won’t significantly impact the dataset’s integrity.\n",
    "\n",
    "**Strategy 2:** Imputation Methods (Fill In Missing Data)\n",
    "\n",
    "If removing data is not an option, impute (i.e., fill in) missing values using statistical measures (e.g., the mean, median, or mode). Imputation allows the model to use all available information, ensuring no data is discarded.\n",
    "This method works well when the data is symmetrically distributed but can introduce bias in skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95abcae1-0bcd-4eae-8bba-2dd48f5020e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Missing Values:\n",
      "      Price  Bedrooms  SquareFeet\n",
      "0  200000.0       3.0      2000.0\n",
      "1  150000.0       2.0      1600.0\n",
      "2       NaN       4.0      2400.0\n",
      "3  130000.0       NaN      1800.0\n",
      "4  250000.0       3.0         NaN\n",
      "\n",
      "DataFrame after Mean Imputation:\n",
      "      Price  Bedrooms  SquareFeet\n",
      "0  200000.0       3.0      2000.0\n",
      "1  150000.0       2.0      1600.0\n",
      "2  182500.0       4.0      2400.0\n",
      "3  130000.0       3.0      1800.0\n",
      "4  250000.0       3.0      1950.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "# Simulated dataset with missing values\n",
    "data = {'Price': [200000, 150000, None, 130000, 250000],\n",
    "        'Bedrooms': [3, 2, 4, None, 3],\n",
    "        'SquareFeet': [2000, 1600, 2400, 1800, None]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "# Impute missing values using the mean for numerical data\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_imputed_mean = pd.DataFrame(imputer_mean.fit_transform(df), columns=df.columns)\n",
    "print(\"\\nDataFrame after Mean Imputation:\")\n",
    "print(df_imputed_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f2323-0a94-4931-8616-4f88fe2f4192",
   "metadata": {},
   "source": [
    "Best Practice. Mean imputation is effective for numerical data with minimal skew. For skewed data, consider using the median instead.\n",
    "\n",
    "**En este caso antes de hacer eso vale la pena ver la distribución de cada variable, con el histograma por ejemplo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab41274-a818-4b44-a819-b82c80e6148e",
   "metadata": {},
   "source": [
    "**Strategy 3:** Mode Imputation for Categorical Data\n",
    "\n",
    "We cannot impute categorical data (e.g., gender or country) using the mean. Instead, we use the mode. Hence, we fill in missing values with the most frequently occurring category.\n",
    "\n",
    "Best Practice. Mode imputation works well for categorical data, especially for features like gender or country, where most frequent values are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccac82c9-6f7b-4425-9df4-6b529491aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Categorical DataFrame with Missing Values:\n",
      "      Name Country\n",
      "0     John     USA\n",
      "1    Emily      UK\n",
      "2  Michael    None\n",
      "3     None     USA\n",
      "4  Jessica  Canada\n",
      "\n",
      "DataFrame after Mode Imputation:\n",
      "      Name Country\n",
      "0     John     USA\n",
      "1    Emily      UK\n",
      "2  Michael     USA\n",
      "3    Emily     USA\n",
      "4  Jessica  Canada\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated dataset with categorical missing values\n",
    "data_cat = {'Name': ['John', 'Emily', 'Michael', None, 'Jessica'],\n",
    "            'Country': ['USA', 'UK', None, 'USA', 'Canada']}\n",
    "df_cat = pd.DataFrame(data_cat)\n",
    "print(\"\\nOriginal Categorical DataFrame with Missing Values:\")\n",
    "print(df_cat)\n",
    "\n",
    "# Replace None with np.nan for proper handling of missing values\n",
    "df_cat.replace({None: np.nan}, inplace=True)\n",
    "\n",
    "# Impute missing values using the mode (most frequent value) for categorical data\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df_cat_imputed = pd.DataFrame(imputer_mode.fit_transform(df_cat), columns=df_cat.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Mode Imputation:\")\n",
    "print(df_cat_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331424c-3cf9-46ab-b8f4-e6b900415106",
   "metadata": {},
   "source": [
    "**Strategy 4:** Advanced Techniques — Multivariate Imputation by Chained Equations (MICE)\n",
    "\n",
    "Simple imputation methods like mean or mode can introduce bias, particularly in complex datasets. We can use Multivariate Imputation by Chained Equations (MICE) for such cases. This technique predicts missing values based on the relationships between multiple features.\n",
    "\n",
    "Best Practice: Use MICE for datasets with complex interdependencies between features, mainly when simple imputations might introduce bias.\n",
    "\n",
    "*Nota: no se puede usar con variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0ee546-fc07-43f0-86b8-8a39513cfbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after MICE Imputation:\n",
      "       Price  Bedrooms  SquareFeet\n",
      "0  200000.00      3.00     2000.00\n",
      "1  150000.00      2.00     1600.00\n",
      "2  254096.39      4.00     2400.00\n",
      "3  130000.00      5.97     1800.00\n",
      "4  250000.00      3.00     2305.46\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Use Iterative Imputer (MICE) for advanced imputation\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "df_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(df), columns=df.columns)\n",
    "print(\"\\nDataFrame after MICE Imputation:\")\n",
    "print(f\"{df_mice_imputed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf26bd-3994-4ab0-8469-5a68e3654728",
   "metadata": {},
   "source": [
    "## 2. Scaling and Normalization\n",
    "\n",
    "Why Scaling and Normalization Are Essential\n",
    "\n",
    "Many machine learning algorithms, especially those involving distance-based metrics (e.g., k-nearest neighbors or support vector machines) or gradient-based optimizers (e.g., logistic regression and neural networks), assume that features are on a similar scale.\n",
    "\n",
    "Theory: What’s the Difference Between Scaling and Normalization?\n",
    "\n",
    "Normalization generally refers to rescaling the data to fall within a specific range, typically [0, 1]. It is often used when the data doesn’t follow a Gaussian distribution.\n",
    "\n",
    "Scaling (aka standardization) refers to adjusting the distribution of values to have a mean of 0 and a standard deviation of 1. It is usually applied to data that follows a Gaussian (i.e., normal) distribution; it is commonly used in algorithms that rely on gradient descent (e.g., logistic regression or neural networks).\n",
    "\n",
    "### Min-Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7106db-e54d-4dad-9c6e-694e232ec5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    Price  Bedrooms  SquareFeet\n",
      "0  200000         3        2000\n",
      "1  150000         2        1600\n",
      "2  180000         4        2400\n",
      "3  130000         3        1800\n",
      "4  250000         3        2200\n",
      "\n",
      "DataFrame after Min-Max Normalization:\n",
      "   Price  Bedrooms  SquareFeet\n",
      "0   0.58       0.5        0.50\n",
      "1   0.17       0.0        0.00\n",
      "2   0.42       1.0        1.00\n",
      "3   0.00       0.5        0.25\n",
      "4   1.00       0.5        0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Simulated dataset\n",
    "data = {'Price': [200000, 150000, 180000, 130000, 250000],\n",
    "        'Bedrooms': [3, 2, 4, 3, 3],\n",
    "        'SquareFeet': [2000, 1600, 2400, 1800, 2200]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max normalization to the dataset\n",
    "df_minmax = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "print(\"\\nDataFrame after Min-Max Normalization:\")\n",
    "print(df_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6c36-f783-4ca8-b48e-40a5bebc4d75",
   "metadata": {},
   "source": [
    "Este método es útil cuando los valores están bien definidos, por ejemplo porcentajes que siempre se muevan entre 0 y 100. Por defecto, el minmax scaler usa los límites según los datos disponibles pero se pueden definir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a66ba-94e9-43ff-972e-693dd208a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset (training data)\n",
    "df_train = pd.DataFrame({\n",
    "    'percentage_score': [55, 70, 85, 90]  # Min value is 55, max value is 90 in training data\n",
    "})\n",
    "limits = [[0], [100]]\n",
    "# Initialize the MinMaxScaler with a fixed range (0, 100)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(limits)  # Fit the scaler using the known min and max (0 and 100)\n",
    "\n",
    "# Normalize the training data\n",
    "df_train['percentage_normalized'] = scaler.transform(df_train[['percentage_score']].values)\n",
    "\n",
    "# Example test set that includes a value outside the training set's range\n",
    "df_test = pd.DataFrame({\n",
    "    'percentage_score': [50, 100, 110]  # Includes a value (50) lower than seen in training and (110) above 100\n",
    "})\n",
    "\n",
    "# Normalize the test data using the fixed scaler\n",
    "df_test['percentage_normalized'] = scaler.transform(df_test[['percentage_score']].values)\n",
    "\n",
    "print(\"Training Data Normalized:\")\n",
    "print(df_train)\n",
    "print(\"\\nTest Data Normalized:\")\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106ec91-04b9-41da-959b-598325719e56",
   "metadata": {},
   "source": [
    "### Z-Score Standardization (Standard Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b27dce-af40-4f5b-b64c-736149cc4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after Z-Score Standardization:\n",
      "   Price  Bedrooms  SquareFeet\n",
      "0   0.43      0.00        0.00\n",
      "1  -0.77     -1.58       -1.41\n",
      "2  -0.05      1.58        1.41\n",
      "3  -1.25      0.00       -0.71\n",
      "4   1.63      0.00        0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "# Apply Z-Score standardization\n",
    "df_std = pd.DataFrame(scaler_std.fit_transform(df), columns=df.columns)\n",
    "print(\"\\nDataFrame after Z-Score Standardization:\")\n",
    "print(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e58455-5c18-41e5-80d1-ab1f07854865",
   "metadata": {},
   "source": [
    "Min-Max Normalization vs. Z-Score Standardization\n",
    "\n",
    "Min-Max Normalization is best when:\n",
    "\n",
    "    Your data does not follow a normal distribution.\n",
    "    Your model makes assumptions about the range of the data (e.g., neural networks with activation functions like sigmoid or tanh, which expect inputs in a specific range).\n",
    "    You want to preserve the relationships between the minimum and maximum values. Furthermore, there are clear upper and lower limits.\n",
    "\n",
    "On the other hand, Z-Score Standardization is best to use when:\n",
    "\n",
    "    Your data follows a Gaussian (i.e., normal) distribution.\n",
    "    You use models like logistic regression, support vector machines, or neural networks that assume standardized inputs for optimal performance.\n",
    "    You need features to be centered around zero, which can prevent issues like slow convergence in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9239e-5fca-4d3f-8247-eb011d565e6d",
   "metadata": {},
   "source": [
    "Potential Pitfalls and Best Practices\n",
    "\n",
    "Outliers.\n",
    "\n",
    "If your data contains significant outliers, Z-score standardization can overinflate their effect because it relies on the mean and standard deviation. Consider removing outliers before standardizing or applying robust scaling techniques.\n",
    "\n",
    "Data Leakage.\n",
    "\n",
    "Always fit your scaler on the training data before applying it to the test set. This prevents data leakage, where information from the test set influences the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc987639-6cd7-4da6-bfc9-8899f1baa1a0",
   "metadata": {},
   "source": [
    "# 3. Encoding Categorical Data\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "Label encoding assigns a unique integer to each category. While this method is simple, it’s mostly suited for ordinal variables (where the categories have an inherent order). It can introduce unintended ordinal relationships for nominal variables (where the categories are unordered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d552395-7e03-4422-bae6-54f526f08ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     education  education_encoded\n",
      "0  High School                  0\n",
      "1   Bachelor's                  1\n",
      "2     Master's                  2\n",
      "3          PhD                  3\n",
      "4   Bachelor's                  1\n",
      "5     Master's                  2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the dataset\n",
    "df = pd.DataFrame({\n",
    "    'education': ['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD', 'Bachelor\\'s', 'Master\\'s']\n",
    "})\n",
    "\n",
    "# Ordinal mapping: explicit order\n",
    "education_order = ['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD']\n",
    "\n",
    "# Apply the LabelEncoder correctly based on the ordinal relationship\n",
    "df['education'] = pd.Categorical(df['education'], categories=education_order, ordered=True)\n",
    "df['education_encoded'] = df['education'].cat.codes\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ac3c5-cab7-47cd-b01d-c8093397bcc7",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5708d724-ccb2-412e-b057-5ec71485c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Categorical DataFrame:\n",
      "   Animal\n",
      "0     Dog\n",
      "1     Cat\n",
      "2  Rabbit\n",
      "3     Dog\n",
      "4  Rabbit\n",
      "\n",
      "DataFrame after One-Hot Encoding:\n",
      "   Animal_Cat  Animal_Dog  Animal_Rabbit\n",
      "0           0           1              0\n",
      "1           1           0              0\n",
      "2           0           0              1\n",
      "3           0           1              0\n",
      "4           0           0              1\n"
     ]
    }
   ],
   "source": [
    "# Simulated dataset with a nominal categorical variable\n",
    "data = {'Animal': ['Dog', 'Cat', 'Rabbit', 'Dog', 'Rabbit']}\n",
    "df_cat = pd.DataFrame(data)\n",
    "print(\"\\nOriginal Categorical DataFrame:\")\n",
    "print(df_cat)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_one_hot = pd.get_dummies(df_cat, columns=['Animal'], prefix='Animal')\n",
    "# change type, though boolean works well for typical code flags and \n",
    "# indexing\n",
    "df_one_hot = df_one_hot.astype(np.uint)\n",
    "print(\"\\nDataFrame after One-Hot Encoding:\")\n",
    "print(df_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64531489-4b6c-4b96-89b7-78f8c50fc0f0",
   "metadata": {},
   "source": [
    "### Advanced Encoding Technique: Target Encoding\n",
    "\n",
    "This method encodes categories based on the mean of the target variable for each category. This is useful in situations where the categorical feature has many levels, but it can introduce overfitting if not done carefully.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "    Target Encoding can be used when a categorical variable has many unique categories (e.g., zip codes, product IDs, or usernames).\n",
    "    This method replaces the categorical values with the mean of the target variable for each category, allowing the model to capture patterns without dramatically increasing the number of features.\n",
    "\n",
    "Caution: Target Encoding can lead to overfitting if not done carefully, especially if the model can memorize the relationship between the category and the target. To mitigate this, techniques like cross-validation or regularization should be applied.\n",
    "\n",
    "# Calculate the mean price per neighborhood\n",
    "mean_target = df_te.groupby('Neighborhood')['Price'].mean()\n",
    "\n",
    "Best Practice. Always apply cross-validation when using Target Encoding to avoid overfitting. A common strategy is to calculate the target encoding on the training data and apply it to the validation/test set to ensure there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35898465-9b59-4e97-b8aa-e67426701d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original DataFrame:\n",
      "  Neighborhood   Price\n",
      "0            A  200000\n",
      "1            B  150000\n",
      "2            A  250000\n",
      "3            C  300000\n",
      "4            B  160000\n",
      "5            C  310000\n",
      "6            A  220000\n",
      "7            B  170000\n",
      "\n",
      "DataFrame after Target Encoding:\n",
      "  Neighborhood   Price  Neighborhood_encoded\n",
      "0            A  200000         223333.333333\n",
      "1            B  150000         160000.000000\n",
      "2            A  250000         223333.333333\n",
      "3            C  300000         305000.000000\n",
      "4            B  160000         160000.000000\n",
      "5            C  310000         305000.000000\n",
      "6            A  220000         223333.333333\n",
      "7            B  170000         160000.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulated dataset with neighborhood and house prices\n",
    "data = {'Neighborhood': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n",
    "        'Price': [200000, 150000, 250000, 300000, 160000, 310000, 220000, 170000]}\n",
    "df_te = pd.DataFrame(data)\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_te)\n",
    "\n",
    "# Calculate the mean price per neighborhood\n",
    "mean_target = df_te.groupby('Neighborhood')['Price'].mean()\n",
    "\n",
    "# Map the mean target encoding back to the original DataFrame\n",
    "df_te['Neighborhood_encoded'] = df_te['Neighborhood'].map(mean_target)\n",
    "print(\"\\nDataFrame after Target Encoding:\")\n",
    "print(df_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbd216-b3fe-4c8e-adc0-d44a752dd3a4",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "The Importance of Feature Engineering\n",
    "\n",
    "Feature engineering is often considered the heart of machine learning, where domain knowledge and creativity intersect to transform raw data into meaningful inputs that improve model performance.\n",
    "\n",
    "## Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6601353d-32e3-4f8f-ad0c-b03a52766ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    Price  Bedrooms  SquareFeet\n",
      "0  200000         3        2000\n",
      "1  150000         2        1600\n",
      "2  180000         4        2400\n",
      "3  130000         3        1800\n",
      "4  250000         3        2200\n",
      "\n",
      "DataFrame after Polynomial Feature Generation:\n",
      "   Bedrooms  SquareFeet  Bedrooms^2  Bedrooms SquareFeet  SquareFeet^2\n",
      "0       3.0      2000.0         9.0               6000.0     4000000.0\n",
      "1       2.0      1600.0         4.0               3200.0     2560000.0\n",
      "2       4.0      2400.0        16.0               9600.0     5760000.0\n",
      "3       3.0      1800.0         9.0               5400.0     3240000.0\n",
      "4       3.0      2200.0         9.0               6600.0     4840000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Simulated dataset\n",
    "data = {'Price': [200000, 150000, 180000, 130000, 250000],\n",
    "        'Bedrooms': [3, 2, 4, 3, 3],\n",
    "        'SquareFeet': [2000, 1600, 2400, 1800, 2200]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize PolynomialFeatures for degree 2\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Apply polynomial transformation\n",
    "df_poly = pd.DataFrame(poly.fit_transform(df[['Bedrooms', 'SquareFeet']]),\n",
    "                       columns=poly.get_feature_names_out(['Bedrooms', 'SquareFeet']))\n",
    "print(\"\\nDataFrame after Polynomial Feature Generation:\")\n",
    "print(df_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cfa1f0-5281-44e4-add9-d3b9a7d7e191",
   "metadata": {},
   "source": [
    "## Log Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db5fd5d-a714-4885-9e7f-5883cf6e239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after Log Transformation of 'Price':\n",
      "    Price  Log_Price\n",
      "0  200000  12.206078\n",
      "1  150000  11.918397\n",
      "2  180000  12.100718\n",
      "3  130000  11.775297\n",
      "4  250000  12.429220\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation to 'Price'\n",
    "df['Log_Price'] = np.log(df['Price'] + 1)  # Adding 1 to avoid log(0)\n",
    "print(\"\\nDataFrame after Log Transformation of 'Price':\")\n",
    "print(df[['Price', 'Log_Price']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68abce-b42c-42d6-811f-50a9cec2b9da",
   "metadata": {},
   "source": [
    "## Binning\n",
    "\n",
    "Binning is dividing continuous variables into intervals (i.e., bins). It is useful when you want to simplify the data or create meaningful groups. This technique constrains a feature’s range or makes the data more interpretable by converting it into categories (e.g., low, medium, and high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb87576c-50b0-4c8c-83d6-dd1fc9d609f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after Binning 'Price':\n",
      "    Price Price_Binned\n",
      "0  200000       Medium\n",
      "1  150000          Low\n",
      "2  180000       Medium\n",
      "3  130000          Low\n",
      "4  250000         High\n"
     ]
    }
   ],
   "source": [
    "# Define bin edges for price categories\n",
    "bins = [0, 150000, 200000, np.inf]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "# Apply binning\n",
    "df['Price_Binned'] = pd.cut(df['Price'], bins=bins, labels=labels)\n",
    "print(\"\\nDataFrame after Binning 'Price':\")\n",
    "print(df[['Price', 'Price_Binned']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697c9e6-bc0a-474a-8907-4804647c567f",
   "metadata": {},
   "source": [
    "## Handling High Cardinality with Feature Hashing\n",
    "\n",
    "Another common problem is when a feature has many unique categories (i.e., high cardinality), such as zip codes, product IDs, or user IDs. Using traditional one-hot encoding in such scenarios can drastically increase the dimensionality of the dataset, leading to memory inefficiencies and longer computation times. We can use feature hashing (i.e., the hashing trick) to reduce dimensionality and preserve essential data patterns.\n",
    "\n",
    "Feature hashing transforms categories into integers using a hash function and assigns them to a fixed number of “buckets” (i.e., columns). This method avoids creating thousands or even millions of one-hot encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7d6f0b-e9a8-400c-ab0b-23c983fae76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hashed Features (Feature Hashing for High Cardinality):\n",
      "   Bucket_1  Bucket_2  Bucket_3  Bucket_4\n",
      "0      -1.0       0.0       0.0       0.0\n",
      "1       0.0       0.0      -1.0       0.0\n",
      "2       0.0       0.0       0.0       1.0\n",
      "3      -1.0       0.0       0.0       0.0\n",
      "4       0.0       1.0       0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Simulated high-cardinality data\n",
    "data = {'ProductID': [['P001'], ['P002'], ['P003'], ['P004'], ['P005']]}\n",
    "df_hash = pd.DataFrame(data)\n",
    "# Feature hashing with 4 output features (buckets)\n",
    "hasher = FeatureHasher(n_features=4, input_type='string')\n",
    "hashed_features = hasher.fit_transform(df_hash['ProductID'])\n",
    "# Convert the hashed result back to a DataFrame\n",
    "df_hashed = pd.DataFrame(hashed_features.toarray(), columns=[f'Bucket_{i}' for i in range(1, 5)])\n",
    "print(\"\\nHashed Features (Feature Hashing for High Cardinality):\")\n",
    "print(df_hashed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e591b-5f68-42a3-b60e-83c20b33dfff",
   "metadata": {},
   "source": [
    "# 5. Dealing with Imbalanced Data\n",
    "\n",
    "Imbalanced data is when one class or label significantly outnumbers the other(s) in a dataset. For example, in fraud detection, the number of fraudulent transactions is usually much smaller than that of non-fraudulent ones. Left untreated, this imbalance can lead to a model that performs well on the majority class but poorly on the minority class.\n",
    "\n",
    "Mathematics and Theory Behind Imbalanced Data\n",
    "\n",
    "When dealing with an imbalanced dataset, there are a few standard metrics and definitions to keep in mind:\n",
    "\n",
    "Imbalance Ratio:\n",
    "The imbalance ratio quantifies the degree of imbalance between the majority and minority classes. For a binary classification problem:\n",
    "\n",
    "IR = # de clase mayoritaria/# de clase minoritaria\n",
    "\n",
    "Ver accuracy, precision, Recall y F1-Score\n",
    "\n",
    "ROC Curve and AUC:\n",
    "The ROC curve plots the true positive rate (recall) against the false positive rate. The AUC (Area Under the Curve) is a standard metric used for imbalanced datasets, evaluating the model’s ability to distinguish between classes irrespective of their distribution.\n",
    "\n",
    "## Technique 1: Class Weighting\n",
    "\n",
    "Class weighting is a standard method used with models that allow you to assign a higher weight to the minority class. By doing so, the model treats errors in the minority class as more costly, encouraging the model to learn from minority examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6590034-e308-42e4-900c-a13d549f582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report with Class Weighting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       178\n",
      "           1       0.92      1.00      0.96        22\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.96      0.99      0.98       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Simulate an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=2,\n",
    "                           n_clusters_per_class=1, weights=[0.9], flip_y=0, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression with class weighting\n",
    "clf = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report with Class Weighting:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f280e2-ddcd-48f0-8c38-3f813313288c",
   "metadata": {},
   "source": [
    "## Technique 2: Random Oversampling\n",
    "\n",
    "Random oversampling involves duplicating instances of the minority class to balance the dataset. It is a simple and effective method, but it can lead to overfitting if the model starts to memorize repeated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9bd648-5e15-4b61-add7-dedb5b41680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after Random Oversampling: [723 723]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "# Apply oversampling to the training data\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "# Check the distribution after oversampling\n",
    "print(\"Class distribution after Random Oversampling:\", np.bincount(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba3209-78a1-4d76-a22d-0665ce95829e",
   "metadata": {},
   "source": [
    "## Technique 3: Random Undersampling\n",
    "\n",
    "Random undersampling involves removing instances of the majority class to balance the dataset. This method can lead to a loss of valuable data from the majority class, but it helps reduce the training time and memory consumption, especially for large datasets.\n",
    "\n",
    "This method is proper when the majority class significantly outnumbers (supera) the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940dc604-05b9-4ff2-9fe6-d3298a29ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after Random Undersampling: [77 77]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Initialize RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "# Apply undersampling to the training data\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "# Check the distribution after undersampling\n",
    "print(\"Class distribution after Random Undersampling:\", np.bincount(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525655e-d51d-4abd-a58a-1e9a1e93728d",
   "metadata": {},
   "source": [
    "## Technique 4: Synthetic Minority Over-sampling Technique (SMOTE)\n",
    "\n",
    "SMOTE is an advanced oversampling technique that creates synthetic instances of the minority class by interpolating between existing examples. This method generates more varied minority class examples, reducing the risk of overfitting compared to random oversampling.\n",
    "\n",
    "SMOTE creates new, synthetic samples of the minority class, balancing the dataset while avoiding the overfitting risk of random oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0fb7b18-e9a4-42e7-95db-8206b6cbb267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: [723 723]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "# Apply SMOTE to the training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "# Check the distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39ce9e-0d8b-4079-bbd2-a89fa8f796a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
